{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration, BartTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909405e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trivia_qa\", \"unfiltered\", split=\"train\", streaming=True)  # subset\n",
    "dataset = list(islice(dataset, 0, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-nq\",\n",
    "    index_name=\"exact\",  # FAISS index trained on DPR-wiki passages\n",
    "    use_dummy_dataset=True  # loads built-in Wikipedia index\n",
    ")\n",
    "\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd42532",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")   \n",
    "generator_tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def process_example(example):\n",
    "    # Tokenize input (question)\n",
    "    input_encodings = rag_tokenizer(example['question'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Use just the \"value\" field of the answer if it exists\n",
    "    if isinstance(example['answer'], dict) and 'value' in example['answer']:\n",
    "        answer_text = example['answer']['value']\n",
    "    else:\n",
    "        answer_text = \"No answer provided\"\n",
    "\n",
    "    # Tokenize answer (target)\n",
    "    target_encodings = generator_tokenizer(answer_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Return tokenized input and target\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5222b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Collate the batch by padding the sequences to the max length in the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "\n",
    "    # Pad sequences to the max length in each batch (or use a fixed size)\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=0)\n",
    "    attention_mask_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in attention_mask], batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in labels], batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'labels': labels_padded,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "accumulation_steps = 8  # Accumulate gradients over 8 steps\n",
    "\n",
    "# Load dataset and process it\n",
    "processed_dataset = [process_example(example) for example in dataset]\n",
    "train_dataloader = DataLoader(processed_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16260e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()  \n",
    "\n",
    "    # Loop over batches\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Compute embeddings for the questions\n",
    "        with torch.no_grad():\n",
    "            output = model.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = output[0]  \n",
    "            question_hidden_states = last_hidden_state.cpu().numpy()  \n",
    "\n",
    "            n_docs = 5  # Number of documents to retrieve\n",
    "            \n",
    "            # Retrieve top 5 docs using embeddings\n",
    "            _, _, doc_dicts = retriever.retrieve(question_hidden_states, n_docs=n_docs)\n",
    "\n",
    "            # Convert doc dicts into context encodings using the generator tokenizer\n",
    "            contexts = [doc[\"text\"] for doc in doc_dicts]\n",
    "\n",
    "            flat_contexts = [item for sublist in contexts for item in sublist]\n",
    "\n",
    "            context_encodings = generator_tokenizer.batch_encode_plus(\n",
    "                flat_contexts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Reshape to [batch_size, n_docs, seq_len]\n",
    "            context_input_ids = context_encodings['input_ids'].view(BATCH_SIZE, n_docs, -1)\n",
    "            context_attention_mask = context_encodings['attention_mask'].view(input_ids.size(0), 5, -1).to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss = outputs.loss.mean()\n",
    "        loss = loss / accumulation_steps  \n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Save the model after training\n",
    "model.save_pretrained(\"rag_model_finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
