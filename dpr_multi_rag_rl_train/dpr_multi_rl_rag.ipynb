{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "# clear cuda memory\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Initialize a Sentence-BERT model for multi-hop retrieval and reward computation.\n",
    "retriever_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL and training hyperparameters\n",
    "LEARNING_RATE = 5e-5\n",
    "num_epochs = 2          # For demonstration; use more epochs in practice.\n",
    "temperature = 0.8\n",
    "max_new_tokens = 64     # Maximum tokens to sample per answer\n",
    "alpha = 0.5             # Weight for supervised loss vs. RL loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(model_answer, gold_answer):\n",
    "    \"\"\"\n",
    "    Compute a reward by comparing the cosine similarity between\n",
    "    embeddings of the model's answer and the gold answer.\n",
    "    \"\"\"\n",
    "    model_embedding = retriever_model.encode(model_answer, convert_to_tensor=True)\n",
    "    gold_embedding = retriever_model.encode(gold_answer, convert_to_tensor=True)\n",
    "    if len(model_embedding.shape) == 1:\n",
    "        model_embedding = model_embedding.unsqueeze(0)\n",
    "    if len(gold_embedding.shape) == 1:\n",
    "        gold_embedding = gold_embedding.unsqueeze(0)\n",
    "    # Higher similarity gives a higher reward.\n",
    "    return cosine_similarity(model_embedding, gold_embedding, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_json(lst, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(lst, file)\n",
    "\n",
    "def rm_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} removed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer.\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "save_file = 'qa_output/mistral_rl_triviaqa.json'\n",
    "# Here we use float32 for stability.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "\n",
    "# Load DPR models and tokenizers\n",
    "dpr_question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
    "dpr_question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "dpr_context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
    "dpr_context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed prompt prefix.\n",
    "prefix = (\n",
    "    \"Below is a question followed by context from different sources. \"\n",
    "    \"Please answer using the context. If insufficient, respond 'Insufficient Information'. \"\n",
    "    \"Answer directly.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b338817062034c0ab2ec81571d587313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1f441ce71f42a8b81a1683442f5c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a small subset of the TriviaQA dataset.\n",
    "dataset = load_dataset(\"trivia_qa\", \"rc\", split=\"train[:100]\")\n",
    "doc_data = []\n",
    "for example in dataset:\n",
    "    query = example[\"question\"]\n",
    "    # Use available evidence if present; otherwise, provide a dummy context.\n",
    "    if \"evidence\" in example and len(example[\"evidence\"]) > 0:\n",
    "        retrieval_list = [{\"text\": example[\"evidence\"][0][\"text\"]}]\n",
    "    else:\n",
    "        retrieval_list = [{\"text\": \"No additional context available.\"}]\n",
    "    # Assume the answer is stored as a string or in a dict.\n",
    "    answer = example[\"answer\"][\"value\"] if isinstance(example[\"answer\"], dict) else example[\"answer\"]\n",
    "    doc_data.append({\n",
    "        \"query\": query,\n",
    "        \"retrieval_list\": retrieval_list,\n",
    "        \"answer\": answer,\n",
    "        \"question_type\": \"trivia\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Optimizer.\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def retrieve_context(query, retrieval_list, hops=2, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval using BM25 + DPR. \n",
    "    - alpha: Weight between BM25 (lexical) and DPR (semantic).\n",
    "    \"\"\"\n",
    "    query_embedding = retriever_model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # BM25 setup\n",
    "    corpus = [doc[\"text\"] for doc in retrieval_list]\n",
    "    tokenized_corpus = [doc.split() for doc in corpus]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    # BM25 scores\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "\n",
    "    # DPR scores\n",
    "    with torch.no_grad():\n",
    "        dpr_query_emb = dpr_question_encoder(**dpr_question_tokenizer(query, return_tensors=\"pt\").to(device)).pooler_output\n",
    "        dpr_doc_embs = []\n",
    "        for doc in corpus:\n",
    "            doc_emb = dpr_context_encoder(**dpr_context_tokenizer(doc, return_tensors=\"pt\").to(device)).pooler_output\n",
    "            dpr_doc_embs.append(doc_emb)\n",
    "        dpr_doc_embs = torch.cat(dpr_doc_embs)\n",
    "        dpr_scores = torch.nn.functional.cosine_similarity(dpr_query_emb, dpr_doc_embs).cpu().numpy()\n",
    "\n",
    "    # Combine scores (weighted sum)\n",
    "    combined_scores = alpha * np.array(bm25_scores) + (1 - alpha) * np.array(dpr_scores)\n",
    "\n",
    "    # Rank documents\n",
    "    ranked_indices = np.argsort(combined_scores)[::-1]\n",
    "    context = \"\\n\\n\".join([corpus[i] for i in ranked_indices[:hops]])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bot_rl(prompt, temperature=temperature, max_new_tokens=max_new_tokens):\n",
    "    \"\"\"\n",
    "    Generate an answer token-by-token using sampling, while accumulating log probabilities.\n",
    "    Returns the generated text and total log probability.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    generated_tokens = []\n",
    "    log_probs = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Convert logits to float32 to avoid NaN issues.\n",
    "        logits = outputs.logits[:, -1, :].to(torch.float32)\n",
    "        logits = logits / temperature\n",
    "        distribution = torch.distributions.Categorical(logits=logits)\n",
    "        token = distribution.sample()  # Sample next token.\n",
    "        log_prob = distribution.log_prob(token)\n",
    "        generated_tokens.append(token)\n",
    "        log_probs.append(log_prob)\n",
    "        # Append the sampled token to the input sequence.\n",
    "        input_ids = torch.cat([input_ids, token.unsqueeze(-1)], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones_like(token.unsqueeze(-1))], dim=-1)\n",
    "        # Stop if EOS token is generated.\n",
    "        if token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    total_log_prob = torch.stack(log_probs).sum()\n",
    "    generated_text = tokenizer.decode(torch.cat(generated_tokens), skip_special_tokens=True)\n",
    "    return generated_text, total_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:06<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:04<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed.\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with Hybrid Training on TriviaQA subset.\n",
    "rm_file(save_file)\n",
    "save_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for d in tqdm(doc_data):\n",
    "        # Retrieve context and construct prompt.\n",
    "        retrieval_list = d['retrieval_list']\n",
    "        context = retrieve_context(d['query'], retrieval_list, hops=2)\n",
    "        prompt = f\"{prefix}\\n\\nQuestion: {d['query']}\\n\\nContext:\\n\\n{context}\"\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # ----- RL Branch -----\n",
    "        # Generate an answer using RL sampling.\n",
    "        rl_response, total_log_prob = query_bot_rl(prompt)\n",
    "        # Compute reward comparing the generated answer with the gold answer.\n",
    "        reward = compute_reward(rl_response, d['answer'])\n",
    "        reward_tensor = torch.tensor(reward, device=device, dtype=total_log_prob.dtype)\n",
    "        rl_loss = -reward_tensor * total_log_prob\n",
    "\n",
    "        # ----- Supervised (MLE) Branch -----\n",
    "        # Construct a combined input: prompt followed by the gold answer.\n",
    "        # We add a marker \"Answer:\" to indicate the beginning of the answer.\n",
    "        prompt_text = prompt + \"\\n\\nAnswer:\"\n",
    "        prompt_enc = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "        answer_enc = tokenizer(d['answer'], return_tensors=\"pt\").to(device)\n",
    "        # Concatenate the prompt and gold answer.\n",
    "        input_ids = torch.cat([prompt_enc.input_ids, answer_enc.input_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_enc.attention_mask, answer_enc.attention_mask], dim=1)\n",
    "        # Create labels: ignore loss for prompt tokens.\n",
    "        labels = input_ids.clone()\n",
    "        labels[:, :prompt_enc.input_ids.shape[1]] = -100\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        supervised_loss = outputs.loss\n",
    "\n",
    "        # ----- Hybrid Loss -----\n",
    "        # Combine supervised (MLE) loss and RL loss.\n",
    "        combined_loss = alpha * supervised_loss + (1 - alpha) * rl_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save training metrics.\n",
    "        save = {\n",
    "            'query': d['query'],\n",
    "            'prompt': prompt,\n",
    "            'rl_model_answer': rl_response,\n",
    "            'gold_answer': d['answer'],\n",
    "            'question_type': d['question_type'],\n",
    "            'reward': reward,\n",
    "            'supervised_loss': supervised_loss.item(),\n",
    "            'rl_loss': rl_loss.item(),\n",
    "            'combined_loss': combined_loss.item()\n",
    "        }\n",
    "        save_list.append(save)\n",
    "        \n",
    "    # Optionally, save a checkpoint after each epoch.\n",
    "    torch.save(model.state_dict(), f\"checkpoint_dpr_multi_rl_epoch_{epoch+1}.pt\")\n",
    "    print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "save_list_to_json(save_list, save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:49<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed.\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoint_dpr_multi_rl_epoch_4.pt\"))\n",
    "# Training Loop with Hybrid Training on TriviaQA subset.\n",
    "save_list = []\n",
    "epoch = 4\n",
    "for d in tqdm(doc_data):\n",
    "    # Retrieve context and construct prompt.\n",
    "    retrieval_list = d['retrieval_list']\n",
    "    context = retrieve_context(d['query'], retrieval_list, hops=2)\n",
    "    prompt = f\"{prefix}\\n\\nQuestion: {d['query']}\\n\\nContext:\\n\\n{context}\"\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # ----- RL Branch -----\n",
    "    # Generate an answer using RL sampling.\n",
    "    rl_response, total_log_prob = query_bot_rl(prompt)\n",
    "    # Compute reward comparing the generated answer with the gold answer.\n",
    "    reward = compute_reward(rl_response, d['answer'])\n",
    "    reward_tensor = torch.tensor(reward, device=device, dtype=total_log_prob.dtype)\n",
    "    rl_loss = -reward_tensor * total_log_prob\n",
    "\n",
    "    # ----- Supervised (MLE) Branch -----\n",
    "    # Construct a combined input: prompt followed by the gold answer.\n",
    "    # We add a marker \"Answer:\" to indicate the beginning of the answer.\n",
    "    prompt_text = prompt + \"\\n\\nAnswer:\"\n",
    "    prompt_enc = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    answer_enc = tokenizer(d['answer'], return_tensors=\"pt\").to(device)\n",
    "    # Concatenate the prompt and gold answer.\n",
    "    input_ids = torch.cat([prompt_enc.input_ids, answer_enc.input_ids], dim=1)\n",
    "    attention_mask = torch.cat([prompt_enc.attention_mask, answer_enc.attention_mask], dim=1)\n",
    "    # Create labels: ignore loss for prompt tokens.\n",
    "    labels = input_ids.clone()\n",
    "    labels[:, :prompt_enc.input_ids.shape[1]] = -100\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    supervised_loss = outputs.loss\n",
    "\n",
    "    # ----- Hybrid Loss -----\n",
    "    # Combine supervised (MLE) loss and RL loss.\n",
    "    combined_loss = alpha * supervised_loss + (1 - alpha) * rl_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    combined_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save training metrics.\n",
    "    save = {\n",
    "        'query': d['query'],\n",
    "        'prompt': prompt,\n",
    "        'rl_model_answer': rl_response,\n",
    "        'gold_answer': d['answer'],\n",
    "        'question_type': d['question_type'],\n",
    "        'reward': reward,\n",
    "        'supervised_loss': supervised_loss.item(),\n",
    "        'rl_loss': rl_loss.item(),\n",
    "        'combined_loss': combined_loss.item()\n",
    "    }\n",
    "    save_list.append(save)\n",
    "    \n",
    "# Optionally, save a checkpoint after each epoch.\n",
    "torch.save(model.state_dict(), f\"checkpoint_dpr_multi_rl_epoch_{epoch+1}.pt\")\n",
    "print(f\"Epoch {epoch+1} completed.\")\n",
    "save_list_to_json(save_list, save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52863f8f2bbd44c3ae161a5a7f9594c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac33ec5bbd9446aa13bba2b3b66b0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:02<00:00,  1.61it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 0.0000\n",
      "BLEU Score: 0.0000\n",
      "ROUGE-L Score: 0.0000\n",
      "BERTScore (F1): 0.9007\n",
      "Average Reward: 0.1622\n",
      "Evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "test_data = load_dataset(\"trivia_qa\", \"rc\", split=\"test[:100]\")\n",
    "\n",
    "doc_data = []\n",
    "for example in test_data:\n",
    "    query = example[\"question\"]\n",
    "    # Use available evidence if present; otherwise, provide a dummy context.\n",
    "    if \"evidence\" in example and len(example[\"evidence\"]) > 0:\n",
    "        retrieval_list = [{\"text\": example[\"evidence\"][0][\"text\"]}]\n",
    "    else:\n",
    "        retrieval_list = [{\"text\": \"No additional context available.\"}]\n",
    "    # Assume the answer is stored as a string or in a dict.\n",
    "    answer = example[\"answer\"][\"value\"] if isinstance(example[\"answer\"], dict) else example[\"answer\"]\n",
    "    doc_data.append({\n",
    "        \"query\": query,\n",
    "        \"retrieval_list\": retrieval_list,\n",
    "        \"answer\": answer,\n",
    "        \"question_type\": \"trivia\"\n",
    "    })\n",
    "\n",
    "# Load evaluation metrics\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "bert_score = load(\"bertscore\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "D = []\n",
    "\n",
    "for data in tqdm(doc_data):\n",
    "    retrieval_list = data['retrieval_list']\n",
    "    context = retrieve_context(data['query'], retrieval_list, hops=2)\n",
    "    prompt = f\"{prefix}\\n\\nQuestion: {data['query']}\\n\\nContext:\\n\\n{context}\"\n",
    "    gold_answer = data['answer']\n",
    "    model_answer, _ = query_bot_rl(prompt)\n",
    "\n",
    "    predictions.append(model_answer)\n",
    "    ground_truths.append(gold_answer)\n",
    "    \n",
    "    reward = compute_reward(model_answer, gold_answer)\n",
    "    D.append(reward)\n",
    "\n",
    "# Compute Exact Match (EM)\n",
    "exact_match = sum([1 if pred.lower().strip() == gold.lower().strip() else 0 \n",
    "                   for pred, gold in zip(predictions, ground_truths)]) / len(ground_truths)\n",
    "\n",
    "# Compute BLEU Score\n",
    "bleu_score = bleu.compute(predictions=predictions, references=ground_truths)\n",
    "\n",
    "# Compute ROUGE Score\n",
    "rouge_score = rouge.compute(predictions=predictions, references=ground_truths)\n",
    "\n",
    "# Compute BERTScore\n",
    "bert_score_result = bert_score.compute(predictions=predictions, references=ground_truths, lang=\"en\")\n",
    "avg_bert_score = sum(bert_score_result['f1']) / len(bert_score_result['f1'])\n",
    "\n",
    "# Compute average reward\n",
    "avg_reward = np.mean(D)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Exact Match (EM): {exact_match:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "print(f\"ROUGE-L Score: {rouge_score['rougeL']:.4f}\")\n",
    "print(f\"BERTScore (F1): {sum(bert_score_result['f1']) / len(bert_score_result['f1']):.4f}\")\n",
    "print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "\n",
    "print(\"Evaluation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
