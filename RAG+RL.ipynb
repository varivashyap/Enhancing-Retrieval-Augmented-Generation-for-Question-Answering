{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52e36541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"trivia_qa\", \"unfiltered\", split=\"train\", streaming=True)  # subset\n",
    "from itertools import islice\n",
    "dataset = islice(dataset, 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ece40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anishkav/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Generating train split: 10000 examples [00:00, 19957.32 examples/s]\n",
      "Generating train split: 10000 examples [00:00, 209148.41 examples/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.06it/s]\n",
      "RagTokenForGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at facebook/rag-token-nq were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-nq\",\n",
    "    index_name=\"exact\",  # FAISS index trained on DPR-wiki passages\n",
    "    use_dummy_dataset=True  # loads built-in Wikipedia index\n",
    ")\n",
    "\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "from transformers import BartTokenizerFast\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "generator_tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475141e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def process_example(example):\n",
    "    # Tokenize input (question)\n",
    "    input_encodings = rag_tokenizer(example['question'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Use just the \"value\" field of the answer if it exists\n",
    "    if isinstance(example['answer'], dict) and 'value' in example['answer']:\n",
    "        answer_text = example['answer']['value']\n",
    "    else:\n",
    "        answer_text = \"No answer provided\"\n",
    "\n",
    "    # Tokenize answer (target)\n",
    "    target_encodings = generator_tokenizer(answer_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Return tokenized input and target\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5a704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Collate the batch by padding the sequences to the max length in the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the max length in each batch (or use a fixed size)\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=0)\n",
    "    attention_mask_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in attention_mask], batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in labels], batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'labels': labels_padded\n",
    "    }\n",
    "\n",
    "processed_dataset = [process_example(example) for example in dataset]\n",
    "\n",
    "# When initializing your DataLoader\n",
    "BATCH_SIZE = 2\n",
    "train_dataloader = DataLoader(processed_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec4a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "/home/anishkav/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from bert_score import score\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Lowercase and remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    gt_tokens = normalize_text(ground_truth).split()\n",
    "    common = set(pred_tokens) & set(gt_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gt_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def compute_reward(predictions, references, device=\"cpu\", alpha=0.3, beta=0.3, gamma=0.4):\n",
    "    \"\"\"\n",
    "    Compute reward as weighted sum of:\n",
    "    alpha * Exact Match + beta * F1 Score + gamma * BERTScore\n",
    "    \"\"\"\n",
    "    # Convert predictions & references to normalized strings for EM/F1\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        em = compute_exact_match(pred, ref)\n",
    "        f1 = compute_f1(pred, ref)\n",
    "        em_scores.append(em)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    em_tensor = torch.tensor(em_scores, dtype=torch.float, device=device)\n",
    "    f1_tensor = torch.tensor(f1_scores, dtype=torch.float, device=device)\n",
    "\n",
    "    # Compute BERTScore F1 (returns a tensor)\n",
    "    _, _, bert_f1 = score(predictions, references, lang='en', verbose=False, device=device)\n",
    "\n",
    "    # Combine all three\n",
    "    reward = alpha * em_tensor + beta * f1_tensor + gamma * bert_f1\n",
    "    return reward  # shape: [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4fb317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  1%|          | 4/500 [01:14<2:00:43, 14.60s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  2%|▏         | 8/500 [02:07<1:44:34, 12.75s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  2%|▏         | 12/500 [03:00<1:39:01, 12.17s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  3%|▎         | 16/500 [03:55<1:39:01, 12.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  4%|▍         | 20/500 [04:50<1:37:43, 12.22s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  5%|▍         | 24/500 [05:47<1:41:27, 12.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  6%|▌         | 28/500 [06:41<1:39:17, 12.62s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  6%|▋         | 32/500 [07:38<1:39:00, 12.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  7%|▋         | 36/500 [08:35<1:38:12, 12.70s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  8%|▊         | 40/500 [09:31<1:37:49, 12.76s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  9%|▉         | 44/500 [10:27<1:36:40, 12.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 10%|▉         | 48/500 [11:25<1:38:02, 13.01s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 10%|█         | 52/500 [12:23<1:38:03, 13.13s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 11%|█         | 56/500 [13:19<1:34:44, 12.80s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 12%|█▏        | 60/500 [14:17<1:34:40, 12.91s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 13%|█▎        | 64/500 [15:14<1:35:57, 13.21s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 14%|█▎        | 68/500 [16:10<1:31:20, 12.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 14%|█▍        | 72/500 [17:09<1:34:14, 13.21s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 15%|█▌        | 76/500 [18:08<1:32:30, 13.09s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 16%|█▌        | 80/500 [19:08<1:35:49, 13.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 17%|█▋        | 84/500 [20:08<1:34:40, 13.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 18%|█▊        | 88/500 [21:08<1:33:29, 13.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 18%|█▊        | 92/500 [22:07<1:30:13, 13.27s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 19%|█▉        | 96/500 [23:07<1:30:35, 13.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 20%|██        | 100/500 [24:06<1:28:35, 13.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 21%|██        | 104/500 [25:05<1:28:52, 13.47s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 22%|██▏       | 108/500 [26:05<1:26:33, 13.25s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 22%|██▏       | 112/500 [27:05<1:28:16, 13.65s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 23%|██▎       | 116/500 [28:05<1:27:11, 13.62s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 24%|██▍       | 120/500 [29:05<1:26:02, 13.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 25%|██▍       | 124/500 [30:05<1:23:18, 13.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 26%|██▌       | 128/500 [31:06<1:24:31, 13.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 26%|██▋       | 132/500 [32:05<1:21:57, 13.36s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 27%|██▋       | 136/500 [33:06<1:23:11, 13.71s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 28%|██▊       | 140/500 [34:05<1:20:13, 13.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 29%|██▉       | 144/500 [35:05<1:20:21, 13.54s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 30%|██▉       | 148/500 [36:01<1:16:52, 13.10s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 30%|███       | 152/500 [37:01<1:17:32, 13.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 31%|███       | 156/500 [38:00<1:15:23, 13.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 32%|███▏      | 160/500 [38:59<1:16:07, 13.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|███▎      | 164/500 [39:56<1:13:06, 13.06s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 34%|███▎      | 168/500 [40:57<1:14:34, 13.48s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 34%|███▍      | 172/500 [41:55<1:12:59, 13.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 35%|███▌      | 176/500 [42:58<1:15:39, 14.01s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 36%|███▌      | 180/500 [43:56<1:10:45, 13.27s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 37%|███▋      | 184/500 [44:55<1:10:37, 13.41s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 38%|███▊      | 188/500 [45:52<1:06:45, 12.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 38%|███▊      | 192/500 [46:51<1:08:05, 13.26s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 39%|███▉      | 196/500 [47:52<1:08:28, 13.51s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 40%|████      | 200/500 [48:50<1:06:11, 13.24s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 41%|████      | 204/500 [49:48<1:04:17, 13.03s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 42%|████▏     | 208/500 [50:48<1:05:47, 13.52s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 42%|████▏     | 212/500 [51:46<1:03:42, 13.27s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 43%|████▎     | 216/500 [52:45<1:03:40, 13.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 44%|████▍     | 220/500 [53:43<1:00:54, 13.05s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 45%|████▍     | 224/500 [54:42<1:01:36, 13.39s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 46%|████▌     | 228/500 [55:40<1:00:02, 13.25s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 46%|████▋     | 232/500 [56:39<59:08, 13.24s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 47%|████▋     | 236/500 [57:36<57:14, 13.01s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 48%|████▊     | 240/500 [58:35<57:48, 13.34s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 49%|████▉     | 244/500 [59:32<55:32, 13.02s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|████▉     | 248/500 [1:00:32<56:03, 13.35s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 252/500 [1:01:29<53:09, 12.86s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 51%|█████     | 256/500 [1:02:26<52:17, 12.86s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 52%|█████▏    | 260/500 [1:03:22<51:38, 12.91s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 53%|█████▎    | 264/500 [1:04:20<51:40, 13.14s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 54%|█████▎    | 268/500 [1:05:17<49:47, 12.88s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 54%|█████▍    | 272/500 [1:06:14<50:03, 13.17s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 55%|█████▌    | 276/500 [1:07:12<48:51, 13.09s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 56%|█████▌    | 280/500 [1:08:11<49:05, 13.39s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 57%|█████▋    | 284/500 [1:09:08<46:02, 12.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 58%|█████▊    | 288/500 [1:10:04<44:59, 12.73s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 58%|█████▊    | 292/500 [1:11:02<45:12, 13.04s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 59%|█████▉    | 296/500 [1:11:59<44:23, 13.06s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 60%|██████    | 300/500 [1:12:54<41:33, 12.47s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 61%|██████    | 304/500 [1:13:51<42:13, 12.93s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 62%|██████▏   | 308/500 [1:14:48<41:07, 12.85s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 62%|██████▏   | 312/500 [1:15:47<41:18, 13.18s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 63%|██████▎   | 316/500 [1:16:43<38:49, 12.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 64%|██████▍   | 320/500 [1:17:42<40:03, 13.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 65%|██████▍   | 324/500 [1:18:40<38:14, 13.04s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 66%|██████▌   | 328/500 [1:19:38<37:50, 13.20s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 66%|██████▋   | 332/500 [1:20:36<36:09, 12.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 67%|██████▋   | 336/500 [1:21:33<35:35, 13.02s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 68%|██████▊   | 340/500 [1:22:28<33:58, 12.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 69%|██████▉   | 344/500 [1:23:26<34:14, 13.17s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 70%|██████▉   | 348/500 [1:24:23<32:40, 12.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 70%|███████   | 352/500 [1:25:21<31:49, 12.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 71%|███████   | 356/500 [1:26:15<29:50, 12.44s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 72%|███████▏  | 360/500 [1:27:14<30:51, 13.23s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 73%|███████▎  | 364/500 [1:28:09<28:41, 12.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 74%|███████▎  | 368/500 [1:29:05<28:18, 12.86s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 74%|███████▍  | 372/500 [1:30:01<27:00, 12.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 75%|███████▌  | 376/500 [1:31:00<27:37, 13.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 76%|███████▌  | 380/500 [1:31:56<25:47, 12.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 77%|███████▋  | 384/500 [1:32:53<24:59, 12.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 78%|███████▊  | 388/500 [1:33:50<24:13, 12.97s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 78%|███████▊  | 392/500 [1:34:49<23:48, 13.23s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 79%|███████▉  | 396/500 [1:35:46<22:18, 12.87s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 80%|████████  | 400/500 [1:36:41<21:04, 12.64s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 81%|████████  | 404/500 [1:37:40<20:48, 13.00s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 82%|████████▏ | 408/500 [1:38:37<19:56, 13.00s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 82%|████████▏ | 412/500 [1:39:33<18:27, 12.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 83%|████████▎ | 416/500 [1:40:30<18:20, 13.10s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 84%|████████▍ | 420/500 [1:41:26<16:56, 12.71s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 85%|████████▍ | 424/500 [1:42:23<16:36, 13.11s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 86%|████████▌ | 428/500 [1:43:20<15:32, 12.95s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 86%|████████▋ | 432/500 [1:44:18<14:59, 13.22s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 87%|████████▋ | 436/500 [1:45:16<13:55, 13.06s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 88%|████████▊ | 440/500 [1:46:14<13:09, 13.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 89%|████████▉ | 444/500 [1:47:13<12:17, 13.18s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 90%|████████▉ | 448/500 [1:48:11<11:19, 13.08s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 90%|█████████ | 452/500 [1:49:07<10:10, 12.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 91%|█████████ | 456/500 [1:50:05<09:45, 13.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 92%|█████████▏| 460/500 [1:51:01<08:28, 12.70s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 93%|█████████▎| 464/500 [1:51:59<07:58, 13.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 94%|█████████▎| 468/500 [1:52:56<06:53, 12.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 94%|█████████▍| 472/500 [1:53:53<06:02, 12.95s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 95%|█████████▌| 476/500 [1:54:50<05:07, 12.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 96%|█████████▌| 480/500 [1:55:48<04:22, 13.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 97%|█████████▋| 484/500 [1:56:47<03:32, 13.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 98%|█████████▊| 488/500 [1:57:45<02:40, 13.36s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 98%|█████████▊| 492/500 [1:58:42<01:43, 12.88s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 99%|█████████▉| 496/500 [1:59:40<00:52, 13.09s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 500/500 [2:00:37<00:00, 12.90s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "100%|██████████| 500/500 [2:00:37<00:00, 14.48s/it]RS_PARALLELISM\n",
      "/home/anishkav/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 50, 'min_length': 1, 'num_beams': 4, 'bad_words_ids': [[0, 0]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Combined Loss: 2.815096113502979\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "rl_every_n_batches = 4  # Compute RL loss every 4 batches\n",
    "\n",
    "alpha = 0.9  # Weight for supervised loss vs RL loss\n",
    "accumulation_steps = 8  # Accumulate gradients over 8 steps\n",
    "\n",
    "for epoch in range(1):  # Training for 1 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()   # new change\n",
    "\n",
    "    # Loop over batches\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # ======== Supervised Loss (no gradients into question encoder) ====\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Compute embeddings for the questions\n",
    "            output = model.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = output[0]  # Since it's a tuple with only one element\n",
    "            question_hidden_states = last_hidden_state.cpu().numpy()  # ✅ Works for 2D tensors\n",
    "\n",
    "            n_docs = 5  # Number of documents to retrieve\n",
    "            # Retrieve top 5 docs using embeddings\n",
    "            _, _, doc_dicts = retriever.retrieve(question_hidden_states, n_docs=n_docs)\n",
    "\n",
    "            # Convert doc dicts into context encodings using the generator tokenizer\n",
    "            contexts = [doc[\"text\"] for doc in doc_dicts]\n",
    "\n",
    "            flat_contexts = [item for sublist in contexts for item in sublist]\n",
    "\n",
    "            context_encodings = generator_tokenizer.batch_encode_plus(\n",
    "                flat_contexts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Reshape to [batch_size, n_docs, seq_len]\n",
    "            context_input_ids = context_encodings['input_ids'].view(BATCH_SIZE, n_docs, -1)\n",
    "            context_attention_mask = context_encodings['attention_mask'].view(input_ids.size(0), 5, -1).to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask\n",
    "        )\n",
    "\n",
    "        supervised_loss = outputs.loss.mean()\n",
    "        \n",
    "        # ======== RL Loss ========\n",
    "        if i % rl_every_n_batches == 0:\n",
    "            # Compute embeddings for the questions\n",
    "            output = model.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = output[0]  # Since it's a tuple with only one element\n",
    "            question_hidden_states = last_hidden_state.detach().cpu().numpy()  # ✅ Works for 2D tensors\n",
    "\n",
    "            n_docs = 5  # Number of documents to retrieve\n",
    "            # Retrieve top 5 docs using embeddings\n",
    "            _, _, doc_dicts = retriever.retrieve(question_hidden_states, n_docs=n_docs)\n",
    "\n",
    "            # Convert doc dicts into context encodings using the generator tokenizer\n",
    "            contexts = [doc[\"text\"] for doc in doc_dicts]\n",
    "\n",
    "            flat_contexts = [item for sublist in contexts for item in sublist]\n",
    "\n",
    "            context_encodings = generator_tokenizer.batch_encode_plus(\n",
    "                flat_contexts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Reshape to [batch_size, n_docs, seq_len]\n",
    "            context_input_ids = context_encodings['input_ids'].view(BATCH_SIZE, n_docs, -1)\n",
    "            context_attention_mask = context_encodings['attention_mask'].view(input_ids.size(0), 5, -1).to(device)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_return_sequences=1,\n",
    "                num_beams=4,\n",
    "                n_docs=4,  # ← So that Batch Size is divisible by n_docs\n",
    "                max_length=16\n",
    "            )\n",
    "\n",
    "            generated_texts = generator_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            reference_texts = generator_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rewards = compute_reward(generated_texts, reference_texts)  # → Tensor of shape [batch_size]\n",
    "            \n",
    "            # Compute log-probs manually\n",
    "            decoder_input_ids = generated_ids[:, :-1].contiguous()\n",
    "            labels_for_logprob = generated_ids[:, 1:].contiguous()\n",
    "\n",
    "            model_outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                context_input_ids=context_input_ids,\n",
    "                context_attention_mask=context_attention_mask,\n",
    "                use_cache=False\n",
    "            )\n",
    "\n",
    "            logits = model_outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_log_probs = torch.gather(log_probs, dim=2, index=labels_for_logprob.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "            # Mask padding\n",
    "            label_mask = (labels_for_logprob != model.config.pad_token_id).float()\n",
    "            sequence_log_probs = (token_log_probs * label_mask).sum(dim=1) / label_mask.sum(dim=1)\n",
    "\n",
    "            rl_loss = (-rewards * sequence_log_probs).mean()\n",
    "\n",
    "        else:\n",
    "            rl_loss = 0.0\n",
    "            \n",
    "        # ======== Combined Loss ========\n",
    "        combined_loss = alpha * supervised_loss + (1 - alpha) * rl_loss\n",
    "        combined_loss = combined_loss / accumulation_steps\n",
    "        combined_loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += combined_loss.item()\n",
    "   \n",
    "    print(f\"Epoch {epoch+1} - Combined Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"rag_model_rl_finetuned\")        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f264c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "k = list(dataset)\n",
    "print(len(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d3c5191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anishkav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation on 100 TriviaQA datapoints ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]/home/anishkav/.local/lib/python3.10/site-packages/transformers/models/rag/tokenization_rag.py:88: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n",
      "Evaluating: 100%|██████████| 100/100 [03:50<00:00,  2.30s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.66 seconds, 27.34 sentences/sec\n",
      "\n",
      "--- Evaluation Metrics ---\n",
      "BLEU: 0.0336\n",
      "ROUGE-1: 0.1377\n",
      "ROUGE-2: 0.0792\n",
      "ROUGE-L: 0.1369\n",
      "Exact Match (EM): 10.00%\n",
      "BERT F1: 0.8674\n",
      "BERT Precision: 0.8683\n",
      "BERT Recall: 0.8689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score_fn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# (If you haven't already downloaded NLTK punkt tokenizer)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Select 100 datapoints for evaluation\n",
    "eval_dataset = random.sample(k, 100)\n",
    "\n",
    "# Initialize metric accumulators\n",
    "bleu_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "em_scores = []\n",
    "\n",
    "# To compute BERTScore in batch later:\n",
    "all_references = []\n",
    "all_hypotheses = []\n",
    "\n",
    "# Initialize ROUGE scorer and BLEU smoothing function\n",
    "rouge_evaluator = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n--- Evaluation on 100 TriviaQA datapoints ---\\n\")\n",
    "for example in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
    "    question = example['question']\n",
    "    ground_truth = example['answer']\n",
    "    # Process ground truth to get a clean string answer\n",
    "    if isinstance(ground_truth, dict) and 'value' in ground_truth:\n",
    "        ref = ground_truth['value']\n",
    "    else:\n",
    "        ref = str(ground_truth)\n",
    "    \n",
    "    # Tokenize question using the RAG tokenizer\n",
    "    input_dict = rag_tokenizer.prepare_seq2seq_batch([question], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_dict['input_ids'],\n",
    "            attention_mask=input_dict['attention_mask'],\n",
    "            max_length=64,\n",
    "            num_beams=4,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    hyp = generator_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Save references and hypotheses for BERTScore later\n",
    "    all_references.append(ref)\n",
    "    all_hypotheses.append(hyp)\n",
    "    \n",
    "    # Compute BLEU score (using whitespace tokenization here)\n",
    "    ref_tokens = ref.split()\n",
    "    hyp_tokens = hyp.split()\n",
    "    bleu = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth_fn)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge_evaluator.score(ref, hyp)\n",
    "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "    \n",
    "    # Compute Exact Match (EM) metric (case-insensitive exact match)\n",
    "    em = 1 if ref.lower().strip() == hyp.lower().strip() else 0\n",
    "    em_scores.append(em)\n",
    "\n",
    "# Compute average metrics\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "avg_em = sum(em_scores) / len(em_scores)\n",
    "\n",
    "# Compute BERTScore (F1) over all examples\n",
    "P, R, F1 = bert_score_fn(all_hypotheses, all_references, lang=\"en\", verbose=True)\n",
    "avg_bert_f1 = F1.mean().item()\n",
    "avg_bert_p = P.mean().item()\n",
    "avg_bert_r = R.mean().item()\n",
    "\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"BLEU: {avg_bleu:.4f}\")\n",
    "print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
    "print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
    "print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
    "print(f\"Exact Match (EM): {avg_em*100:.2f}%\")\n",
    "print(f\"BERT F1: {avg_bert_f1:.4f}\")\n",
    "print(f\"BERT Precision: {avg_bert_p:.4f}\")\n",
    "print(f\"BERT Recall: {avg_bert_r:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
