{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration, BartTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083db557",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trivia_qa\", \"unfiltered\", split=\"train\", streaming=True) \n",
    "dataset = list(islice(dataset, 0, 1000)) # Limit to 1000 samples when space is limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-nq\",\n",
    "    index_name=\"exact\",  # FAISS index trained on DPR-wiki passages\n",
    "    use_dummy_dataset=True  # loads built-in Wikipedia index\n",
    ")\n",
    "\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")   \n",
    "generator_tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def process_example(example):\n",
    "    # Tokenize input (question)\n",
    "    input_encodings = rag_tokenizer(example['question'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Use just the \"value\" field of the answer if it exists\n",
    "    if isinstance(example['answer'], dict) and 'value' in example['answer']:\n",
    "        answer_text = example['answer']['value']\n",
    "    else:\n",
    "        answer_text = \"No answer provided\"\n",
    "\n",
    "    # Tokenize answer (target)\n",
    "    target_encodings = generator_tokenizer(answer_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Return tokenized input and target\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Collate the batch by padding the sequences to the max length in the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "\n",
    "    # Pad sequences to the max length in each batch (or use a fixed size)\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=0)\n",
    "    attention_mask_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in attention_mask], batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in labels], batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'labels': labels_padded,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450953be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "accumulation_steps = 8  # Accumulate gradients over 8 steps\n",
    "\n",
    "# Load dataset and process it\n",
    "processed_dataset = [process_example(example) for example in dataset]\n",
    "train_dataloader = DataLoader(processed_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dced75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()  \n",
    "    \n",
    "    # Loop over batches\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Compute embeddings for the questions\n",
    "        with torch.no_grad():\n",
    "            output = model.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = output[0]  \n",
    "            question_hidden_states = last_hidden_state.cpu().numpy()  \n",
    "\n",
    "        # üîÅ Multihop change: Retrieve top-k documents and concatenate them into a single context\n",
    "        n_docs = 5  # Number of documents per hop\n",
    "        n_hops = 2  # Number of hops (i.e., chained sets of documents)\n",
    "        all_contexts = []\n",
    "\n",
    "        # Retrieve contexts for multiple hops\n",
    "        question_hidden_states_hop = question_hidden_states\n",
    "        for hop in range(n_hops):\n",
    "            _, _, doc_dicts = retriever.retrieve(question_hidden_states_hop, n_docs=n_docs)\n",
    "            hop_contexts = [doc[\"text\"] for doc in doc_dicts]\n",
    "            hop_contexts = [item for sublist in hop_contexts for item in sublist]\n",
    "            all_contexts.append(hop_contexts)\n",
    "\n",
    "            # For the next hop, re-encode the concatenated docs from this hop as the new query\n",
    "            flat_hop_contexts = [\" \".join(docs) for docs in hop_contexts]\n",
    "            hop_encodings = rag_tokenizer(flat_hop_contexts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                hop_output = model.question_encoder(input_ids=hop_encodings['input_ids'].to(device),\n",
    "                                                    attention_mask=hop_encodings['attention_mask'].to(device))\n",
    "                question_hidden_states_hop = hop_output[0].cpu().numpy()\n",
    "\n",
    "        # Concatenate all retrieved documents from both hops\n",
    "        combined_contexts = [\" \".join(docs) for docs in all_contexts]\n",
    "\n",
    "        context_encodings = generator_tokenizer.batch_encode_plus(\n",
    "            combined_contexts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        context_input_ids = context_encodings['input_ids'].to(device).unsqueeze(1)  # shape: [B, 1, seq_len]\n",
    "        context_attention_mask = context_encodings['attention_mask'].to(device).unsqueeze(1)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss.mean()\n",
    "\n",
    "        loss = loss / accumulation_steps  \n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# === Save Model ===\n",
    "model.save_pretrained(\"rag_model_finetuned_multihop\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
