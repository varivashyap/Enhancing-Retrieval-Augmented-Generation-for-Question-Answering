{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e36541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anishkav/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"trivia_qa\", \"unfiltered\", split=\"train\", streaming=True)  # subset\n",
    "from itertools import islice\n",
    "dataset = islice(dataset, 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ece40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anishkav/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "RagTokenForGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at facebook/rag-token-nq were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-nq\",\n",
    "    index_name=\"exact\",  # FAISS index trained on DPR-wiki passages\n",
    "    use_dummy_dataset=True  # loads built-in Wikipedia index\n",
    ")\n",
    "\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "from transformers import BartTokenizerFast\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "generator_tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475141e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def process_example(example):\n",
    "    # Tokenize input (question)\n",
    "    input_encodings = rag_tokenizer(example['question'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Use just the \"value\" field of the answer if it exists\n",
    "    if isinstance(example['answer'], dict) and 'value' in example['answer']:\n",
    "        answer_text = example['answer']['value']\n",
    "    else:\n",
    "        answer_text = \"No answer provided\"\n",
    "\n",
    "    # Tokenize answer (target)\n",
    "    target_encodings = generator_tokenizer(answer_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Return tokenized input and target\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5a704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Collate the batch by padding the sequences to the max length in the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the max length in each batch (or use a fixed size)\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=0)\n",
    "    attention_mask_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in attention_mask], batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in labels], batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'labels': labels_padded\n",
    "    }\n",
    "\n",
    "processed_dataset = [process_example(example) for example in dataset]\n",
    "\n",
    "# When initializing your DataLoader\n",
    "BATCH_SIZE = 2\n",
    "train_dataloader = DataLoader(processed_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec4a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anishkav/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from bert_score import score\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Lowercase and remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    gt_tokens = normalize_text(ground_truth).split()\n",
    "    common = set(pred_tokens) & set(gt_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gt_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# def compute_reward(predictions, references, device=\"cpu\", alpha=0.3, beta=0.3, gamma=0.4):\n",
    "#     \"\"\"\n",
    "#     Compute reward as weighted sum of:\n",
    "#     alpha * Exact Match + beta * F1 Score + gamma * BERTScore\n",
    "#     \"\"\"\n",
    "#     # Convert predictions & references to normalized strings for EM/F1\n",
    "#     em_scores = []\n",
    "#     f1_scores = []\n",
    "#     for pred, ref in zip(predictions, references):\n",
    "#         em = compute_exact_match(pred, ref)\n",
    "#         f1 = compute_f1(pred, ref)\n",
    "#         em_scores.append(em)\n",
    "#         f1_scores.append(f1)\n",
    "\n",
    "#     em_tensor = torch.tensor(em_scores, dtype=torch.float, device=device)\n",
    "#     f1_tensor = torch.tensor(f1_scores, dtype=torch.float, device=device)\n",
    "\n",
    "#     # Compute BERTScore F1 (returns a tensor)\n",
    "#     _, _, bert_f1 = score(predictions, references, lang='en', verbose=False, device=device)\n",
    "\n",
    "#     # Combine all three\n",
    "#     reward = alpha * em_tensor + beta * f1_tensor + gamma * bert_f1\n",
    "#     return reward  # shape: [batch_size]\n",
    "\n",
    "def compute_reward(predictions, references, retrieved_contexts, device=\"cpu\", alpha=0.25, beta=0.25, gamma=0.25, delta=0.25):\n",
    "    \"\"\"\n",
    "    Compute reward as weighted sum of:\n",
    "    alpha * Exact Match + beta * F1 Score + gamma * BERTScore + delta * Consistency Score\n",
    "    \"\"\"\n",
    "\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        em = compute_exact_match(pred, ref)\n",
    "        f1 = compute_f1(pred, ref)\n",
    "        em_scores.append(em)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    em_tensor = torch.tensor(em_scores, dtype=torch.float, device=device)\n",
    "    f1_tensor = torch.tensor(f1_scores, dtype=torch.float, device=device)\n",
    "\n",
    "    # Compute BERTScore between prediction and reference\n",
    "    _, _, bert_f1 = score(predictions, references, lang='en', verbose=False, device=device)\n",
    "\n",
    "    # Compute Consistency: BERTScore between prediction and retrieved contexts\n",
    "    # retrieved_contexts: list of list of strings (for each sample)\n",
    "    consistency_scores = []\n",
    "    for pred, contexts in zip(predictions, retrieved_contexts):\n",
    "        P, R, F1 = score([pred], [contexts], lang='en', verbose=False, device=device)\n",
    "        consistency_scores.append(F1.item())\n",
    "\n",
    "    consistency_tensor = torch.tensor(consistency_scores, dtype=torch.float, device=device)\n",
    "\n",
    "    # Final reward\n",
    "    reward = alpha * em_tensor + beta * f1_tensor + gamma * bert_f1 + delta * consistency_tensor\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "rl_every_n_batches = 4  # Compute RL loss every 4 batches\n",
    "\n",
    "alpha = 0.9  # Weight for supervised loss vs RL loss\n",
    "accumulation_steps = 8  # Accumulate gradients over 8 steps\n",
    "\n",
    "for epoch in range(1):  # Training for 1 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()   # new change\n",
    "\n",
    "    # Loop over batches\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # ======== Supervised Loss (no gradients into question encoder) ====\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Compute embeddings for the questions\n",
    "            output = model.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = output[0]  # Since it's a tuple with only one element\n",
    "            question_hidden_states = last_hidden_state.cpu().numpy()  # ‚úÖ Works for 2D tensors\n",
    "\n",
    "            n_docs = 5  # Number of documents to retrieve\n",
    "            # Retrieve top 5 docs using embeddings\n",
    "            _, _, doc_dicts = retriever.retrieve(question_hidden_states, n_docs=n_docs)\n",
    "\n",
    "            # Convert doc dicts into context encodings using the generator tokenizer\n",
    "            contexts = [doc[\"text\"] for doc in doc_dicts]\n",
    "\n",
    "            flat_contexts = [item for sublist in contexts for item in sublist]\n",
    "\n",
    "            context_encodings = generator_tokenizer.batch_encode_plus(\n",
    "                flat_contexts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Reshape to [batch_size, n_docs, seq_len]\n",
    "            context_input_ids = context_encodings['input_ids'].view(BATCH_SIZE, n_docs, -1)\n",
    "            context_attention_mask = context_encodings['attention_mask'].view(input_ids.size(0), 5, -1).to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask\n",
    "        )\n",
    "\n",
    "        supervised_loss = outputs.loss.mean()\n",
    "        \n",
    "        # ======== RL Loss ========\n",
    "        if i % rl_every_n_batches == 0:\n",
    "            # Compute embeddings for the questions\n",
    "            output = model.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = output[0]  # Since it's a tuple with only one element\n",
    "            question_hidden_states = last_hidden_state.detach().cpu().numpy()  # ‚úÖ Works for 2D tensors\n",
    "\n",
    "            n_docs = 5  # Number of documents to retrieve\n",
    "            # Retrieve top 5 docs using embeddings\n",
    "            _, _, doc_dicts = retriever.retrieve(question_hidden_states, n_docs=n_docs)\n",
    "\n",
    "            # Convert doc dicts into context encodings using the generator tokenizer\n",
    "            contexts = [doc[\"text\"] for doc in doc_dicts]\n",
    "\n",
    "            flat_contexts = [item for sublist in contexts for item in sublist]\n",
    "\n",
    "            context_encodings = generator_tokenizer.batch_encode_plus(\n",
    "                flat_contexts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Reshape to [batch_size, n_docs, seq_len]\n",
    "            context_input_ids = context_encodings['input_ids'].view(BATCH_SIZE, n_docs, -1)\n",
    "            context_attention_mask = context_encodings['attention_mask'].view(input_ids.size(0), 5, -1).to(device)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_return_sequences=1,\n",
    "                num_beams=4,\n",
    "                n_docs=4,  # ‚Üê So that Batch Size is divisible by n_docs\n",
    "                max_length=16\n",
    "            )\n",
    "\n",
    "            generated_texts = generator_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            reference_texts = generator_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Prepare retrieved contexts\n",
    "            retrieved_contexts = []\n",
    "            for docs in doc_dicts:  # doc_dicts is batch of [n_docs retrieved per question]\n",
    "                contexts = [doc[\"text\"] for doc in doc_dicts]\n",
    "                flat_contexts = [item for sublist in contexts for item in sublist]\n",
    "                retrieved_contexts.append(flat_contexts)\n",
    "\n",
    "            rewards = compute_reward(\n",
    "                generated_texts, \n",
    "                reference_texts, \n",
    "                retrieved_contexts=retrieved_contexts, \n",
    "                device=device)\n",
    "\n",
    "            #rewards = compute_reward(generated_texts, reference_texts)  # ‚Üí Tensor of shape [batch_size]\n",
    "            \n",
    "            # Compute log-probs manually\n",
    "            decoder_input_ids = generated_ids[:, :-1].contiguous()\n",
    "            labels_for_logprob = generated_ids[:, 1:].contiguous()\n",
    "\n",
    "            model_outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                context_input_ids=context_input_ids,\n",
    "                context_attention_mask=context_attention_mask,\n",
    "                use_cache=False\n",
    "            )\n",
    "\n",
    "            logits = model_outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_log_probs = torch.gather(log_probs, dim=2, index=labels_for_logprob.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "            # Mask padding\n",
    "            label_mask = (labels_for_logprob != model.config.pad_token_id).float()\n",
    "            sequence_log_probs = (token_log_probs * label_mask).sum(dim=1) / label_mask.sum(dim=1)\n",
    "\n",
    "            rl_loss = (-rewards * sequence_log_probs).mean()\n",
    "\n",
    "        else:\n",
    "            rl_loss = 0.0\n",
    "            \n",
    "        # ======== Combined Loss ========\n",
    "        combined_loss = alpha * supervised_loss + (1 - alpha) * rl_loss\n",
    "        combined_loss = combined_loss / accumulation_steps\n",
    "        combined_loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += combined_loss.item()\n",
    "   \n",
    "    print(f\"Epoch {epoch+1} - Combined Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"rag_model_new_rl_finetuned\")        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afc8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trivia_qa\", \"unfiltered\", split=\"train\", streaming=True)  # subset\n",
    "eval_dataset = list(islice(dataset, 0, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f264c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example_eval(example):\n",
    "    # Tokenize input (question)\n",
    "    input_encodings = rag_tokenizer(example['question'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Use just the \"value\" field of the answer if it exists\n",
    "    if isinstance(example['answer'], dict) and 'value' in example['answer']:\n",
    "        answer_text = example['answer']['value']\n",
    "    else:\n",
    "        answer_text = \"No answer provided\"\n",
    "\n",
    "    # Tokenize answer (target)\n",
    "    target_encodings = generator_tokenizer(answer_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Return tokenized input and target\n",
    "    return {\n",
    "        'question': example['question'],        \n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'answer': answer_text\n",
    "    }\n",
    "    \n",
    "processed_eval_dataset = [process_example_eval(example) for example in eval_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb1569d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "new_model = RagTokenForGeneration.from_pretrained(\"multihop_rl_finetuned\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b00a19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation on 500 TriviaQA datapoints ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [07:56<00:00,  4.77s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 302.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.73 seconds, 137.11 sentences/sec\n",
      "\n",
      "--- Evaluation Metrics ---\n",
      "BLEU: 0.0000\n",
      "ROUGE-1: 0.0000\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0000\n",
      "Exact Match (EM): 0.00%\n",
      "BERT F1: 0.8171\n",
      "BERT Precision: 0.8083\n",
      "BERT Recall: 0.8295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score_fn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# (If you haven't already downloaded NLTK punkt tokenizer)\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Initialize metric accumulators\n",
    "bleu_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "em_scores = []\n",
    "\n",
    "# To compute BERTScore in batch later:\n",
    "all_references = []\n",
    "all_hypotheses = []\n",
    "\n",
    "# Initialize ROUGE scorer and BLEU smoothing function\n",
    "rouge_evaluator = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n--- Evaluation on 500 TriviaQA datapoints ---\\n\")\n",
    "for example in tqdm(processed_eval_dataset, desc=\"Evaluating\"):\n",
    "    question = example['question']\n",
    "    ref = example['answer']\n",
    "    input_ids = torch.tensor([example['input_ids']], device=device)\n",
    "    attention_mask = torch.tensor([example['attention_mask']], device=device)\n",
    "\n",
    "    \n",
    "    # Compute embeddings for the questions\n",
    "    with torch.no_grad():\n",
    "        output = model.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output[0]  # Since it's a tuple with only one element\n",
    "        question_hidden_states = last_hidden_state.cpu().numpy()  # ‚úÖ Works for 2D tensors\n",
    "\n",
    "    n_docs = 5  # Number of documents per hop\n",
    "    _, _, doc_dicts = retriever.retrieve(question_hidden_states, n_docs=n_docs)\n",
    "\n",
    "    # Flatten list of document texts\n",
    "    retrieved_docs = [doc[\"text\"] for doc in doc_dicts]\n",
    "    flat_docs = [item for sublist in retrieved_docs for item in sublist]\n",
    "    \n",
    "    # Encode retrieved documents\n",
    "    context_encodings = generator_tokenizer.batch_encode_plus(\n",
    "        flat_docs,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Expand dimensions to [batch_size * n_docs, seq_len]\n",
    "    context_input_ids = context_encodings['input_ids'].to(device)\n",
    "    context_attention_mask = context_encodings['attention_mask'].to(device)\n",
    "\n",
    "    # Ensure correct shapes: context_input_ids shape should be [batch_size * n_docs, seq_len]\n",
    "    total_docs = context_input_ids.shape[0]\n",
    "    assert total_docs == n_docs, f\"Expected {n_docs} docs, got {total_docs}\"\n",
    "\n",
    "    doc_scores = torch.ones(1, n_docs, device=device)  # Shape: [1, n_docs]\n",
    "\n",
    "    # Generate answer\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        context_input_ids=context_input_ids,\n",
    "        context_attention_mask=context_attention_mask,\n",
    "        doc_scores=doc_scores,\n",
    "        n_docs=n_docs,\n",
    "        max_length=64,\n",
    "        num_beams=4\n",
    "    )\n",
    "        \n",
    "    hyp = generator_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Save references and hypotheses for BERTScore later\n",
    "    all_references.append(ref)\n",
    "    all_hypotheses.append(hyp)\n",
    "    \n",
    "    # Compute BLEU score (using whitespace tokenization here)\n",
    "    ref_tokens = ref.split()\n",
    "    hyp_tokens = hyp.split()\n",
    "    bleu = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth_fn)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge_evaluator.score(ref, hyp)\n",
    "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "    \n",
    "    # Compute Exact Match (EM) metric (case-insensitive exact match)\n",
    "    em = 1 if ref.lower().strip() == hyp.lower().strip() else 0\n",
    "    em_scores.append(em)\n",
    "\n",
    "# Compute average metrics\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "avg_em = sum(em_scores) / len(em_scores)\n",
    "\n",
    "# Compute BERTScore (F1) over all examples\n",
    "P, R, F1 = bert_score_fn(all_hypotheses, all_references, lang=\"en\", verbose=True)\n",
    "avg_bert_f1 = F1.mean().item()\n",
    "avg_bert_p = P.mean().item()\n",
    "avg_bert_r = R.mean().item()\n",
    "\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"BLEU: {avg_bleu:.4f}\")\n",
    "print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
    "print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
    "print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
    "print(f\"Exact Match (EM): {avg_em*100:.2f}%\")\n",
    "print(f\"BERT F1: {avg_bert_f1:.4f}\")\n",
    "print(f\"BERT Precision: {avg_bert_p:.4f}\")\n",
    "print(f\"BERT Recall: {avg_bert_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9990930",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_dataset(\"trivia_qa\", \"unfiltered\", split=\"validation\", streaming=True)  # subset\n",
    "eval_dataset = islice(eval_dataset, 0, 100)\n",
    "eval_dataset = list(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d3c5191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anishkav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation on 100 TriviaQA datapoints ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]/home/anishkav/.local/lib/python3.10/site-packages/transformers/models/rag/tokenization_rag.py:88: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ü§ó Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [04:19<00:00,  2.59s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 260.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.77 seconds, 129.78 sentences/sec\n",
      "\n",
      "--- Evaluation Metrics ---\n",
      "BLEU: 0.0098\n",
      "ROUGE-1: 0.0593\n",
      "ROUGE-2: 0.0033\n",
      "ROUGE-L: 0.0593\n",
      "Exact Match (EM): 4.00%\n",
      "BERT F1: 0.8730\n",
      "BERT Precision: 0.8809\n",
      "BERT Recall: 0.8675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score_fn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# (If you haven't already downloaded NLTK punkt tokenizer)\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Initialize metric accumulators\n",
    "bleu_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "em_scores = []\n",
    "\n",
    "# To compute BERTScore in batch later:\n",
    "all_references = []\n",
    "all_hypotheses = []\n",
    "\n",
    "# Initialize ROUGE scorer and BLEU smoothing function\n",
    "rouge_evaluator = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n--- Evaluation on 100 TriviaQA datapoints ---\\n\")\n",
    "for example in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
    "    question = example['question']\n",
    "    ground_truth = example['answer']\n",
    "    # Process ground truth to get a clean string answer\n",
    "    if isinstance(ground_truth, dict) and 'value' in ground_truth:\n",
    "        ref = ground_truth['value']\n",
    "    else:\n",
    "        ref = str(ground_truth)\n",
    "    \n",
    "    # Tokenize question using the RAG tokenizer\n",
    "    input_dict = rag_tokenizer.prepare_seq2seq_batch([question], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_dict['input_ids'],\n",
    "            attention_mask=input_dict['attention_mask'],\n",
    "            max_length=64,\n",
    "            num_beams=4,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    hyp = generator_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Save references and hypotheses for BERTScore later\n",
    "    all_references.append(ref)\n",
    "    all_hypotheses.append(hyp)\n",
    "    \n",
    "    # Compute BLEU score (using whitespace tokenization here)\n",
    "    ref_tokens = ref.split()\n",
    "    hyp_tokens = hyp.split()\n",
    "    bleu = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth_fn)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge_evaluator.score(ref, hyp)\n",
    "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "    \n",
    "    # Compute Exact Match (EM) metric (case-insensitive exact match)\n",
    "    em = 1 if ref.lower().strip() == hyp.lower().strip() else 0\n",
    "    em_scores.append(em)\n",
    "\n",
    "# Compute average metrics\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "avg_em = sum(em_scores) / len(em_scores)\n",
    "\n",
    "# Compute BERTScore (F1) over all examples\n",
    "P, R, F1 = bert_score_fn(all_hypotheses, all_references, lang=\"en\", verbose=True)\n",
    "avg_bert_f1 = F1.mean().item()\n",
    "avg_bert_p = P.mean().item()\n",
    "avg_bert_r = R.mean().item()\n",
    "\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"BLEU: {avg_bleu:.4f}\")\n",
    "print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
    "print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
    "print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
    "print(f\"Exact Match (EM): {avg_em*100:.2f}%\")\n",
    "print(f\"BERT F1: {avg_bert_f1:.4f}\")\n",
    "print(f\"BERT Precision: {avg_bert_p:.4f}\")\n",
    "print(f\"BERT Recall: {avg_bert_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10bf43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
