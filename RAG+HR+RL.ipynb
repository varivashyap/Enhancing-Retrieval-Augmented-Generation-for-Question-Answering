{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration, BartTokenizerFast\n",
    "from bert_score import score\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "from torch.nn import functional as F\n",
    "from rank_bm25 import BM25Okapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42360efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trivia_qa\", \"unfiltered\", split=\"train\") \n",
    "\n",
    "# Initialize Dense Retriever\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-nq\",\n",
    "    index_name=\"exact\",  # FAISS index trained on DPR-wiki passages\n",
    "    use_dummy_dataset=True  # loads built-in Wikipedia index\n",
    ")\n",
    "\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")   \n",
    "generator_tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "dense_corpus = retriever.index.dataset  # This holds the corpus of documents for retrieval\n",
    "\n",
    "# Initialize sparse retriever (e.g., BM25) using the same dataset\n",
    "sparse_corpus = [doc[\"text\"].split() for doc in dense_corpus]  # Pre-tokenize the documents for BM25\n",
    "bm25 = BM25Okapi(sparse_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def process_example(example):\n",
    "    # Tokenize input (question)\n",
    "    input_encodings = rag_tokenizer(example['question'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Use just the \"value\" field of the answer if it exists\n",
    "    if isinstance(example['answer'], dict) and 'value' in example['answer']:\n",
    "        answer_text = example['answer']['value']\n",
    "    else:\n",
    "        answer_text = \"No answer provided\"\n",
    "\n",
    "    # Tokenize answer (target)\n",
    "    target_encodings = generator_tokenizer(answer_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Return tokenized input and target\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Collate the batch by padding the sequences to the max length in the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "\n",
    "    # Pad sequences to the max length in each batch (or use a fixed size)\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=0)\n",
    "    attention_mask_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in attention_mask], batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in labels], batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'labels': labels_padded,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b1ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Lowercase and remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    gt_tokens = normalize_text(ground_truth).split()\n",
    "    common = set(pred_tokens) & set(gt_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gt_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def compute_reward(predictions, references, retrieved_contexts, device, alpha=0.2, beta=0.4, gamma=0.3, delta=0.1):\n",
    "    \"\"\"\n",
    "    Compute reward as weighted sum of:\n",
    "    alpha * Exact Match + beta * F1 Score + gamma * BERTScore + delta * Consistency Score\n",
    "    \"\"\"\n",
    "\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        em = compute_exact_match(pred, ref)\n",
    "        f1 = compute_f1(pred, ref)\n",
    "        em_scores.append(em)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    em_tensor = torch.tensor(em_scores, dtype=torch.float, device=device)\n",
    "    f1_tensor = torch.tensor(f1_scores, dtype=torch.float, device=device)\n",
    "\n",
    "    # Compute BERTScore between prediction and reference\n",
    "    _, _, bert_f1 = score(predictions, references, lang='en', verbose=False, device=device)\n",
    "\n",
    "    # Compute Consistency: BERTScore between prediction and retrieved contexts\n",
    "    # retrieved_contexts: list of list of strings (for each sample)\n",
    "    consistency_scores = []\n",
    "    for pred, contexts in zip(predictions, retrieved_contexts):\n",
    "        P, R, F1 = score([pred], [contexts], lang='en', verbose=False, device=device)\n",
    "        consistency_scores.append(F1.item())\n",
    "\n",
    "    consistency_tensor = torch.tensor(consistency_scores, dtype=torch.float, device=device)\n",
    "\n",
    "    # Final reward\n",
    "    reward = alpha * em_tensor + beta * f1_tensor + gamma * bert_f1 + delta * consistency_tensor\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ec79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "S = 0.7\n",
    "D = 0.3\n",
    "accumulation_steps = 8  # Accumulate gradients over 8 steps\n",
    "rl_every_n_batches = 4  # Compute RL loss every 4 batches\n",
    "Lambda = 0.9  # Weight for supervised loss vs RL loss\n",
    "\n",
    "# Load dataset and process it\n",
    "processed_dataset = [process_example(example) for example in dataset]\n",
    "train_dataloader = DataLoader(processed_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31719f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):  # Training for 1 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()   \n",
    "\n",
    "    # Loop over batches\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # ======== Supervised Loss (no gradients into question encoder) ====\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Compute embeddings for the questions\n",
    "            output = model.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = output[0]  \n",
    "            question_hidden_states = last_hidden_state.cpu().numpy()  \n",
    "            \n",
    "            n_docs = 5  # Number of documents to retrieve\n",
    "            \n",
    "            # Retrieve top 5 docs using embeddings\n",
    "            _, _, doc_dicts = retriever.retrieve(question_hidden_states, n_docs=n_docs)\n",
    "\n",
    "            query_terms = [rag_tokenizer.decode(ids, skip_special_tokens=True).split() for ids in input_ids]\n",
    "            sparse_doc_dicts = []\n",
    "            for query in query_terms:\n",
    "                scores = bm25.get_scores(query)\n",
    "                top_doc_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:n_docs]\n",
    "                sparse_doc_dicts.extend([sparse_corpus[i] for i in top_doc_indices])\n",
    "\n",
    "            # Combine dense and sparse retrieved documents\n",
    "            sparse_doc_dicts = [doc for doc in sparse_doc_dicts if doc]  # Filter out empty documents\n",
    "            sparse_doc_dicts = sparse_doc_dicts[:int(S * len(sparse_doc_dicts))]\n",
    "            dense_doc_dicts = [doc[\"text\"] for doc in doc_dicts if doc]  # Filter out empty documents\n",
    "            dense_doc_dicts = dense_doc_dicts[:int(D * len(dense_doc_dicts))]\n",
    "            combined_contexts = dense_doc_dicts + sparse_doc_dicts\n",
    "            flat_combined_contexts = [item for sublist in combined_contexts for item in sublist]\n",
    "\n",
    "            context_encodings = generator_tokenizer.batch_encode_plus(\n",
    "                flat_combined_contexts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Reshape to [batch_size, n_docs, seq_len]\n",
    "            context_input_ids = context_encodings['input_ids'].view(BATCH_SIZE, n_docs, -1)\n",
    "            context_attention_mask = context_encodings['attention_mask'].view(input_ids.size(0), 5, -1).to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask\n",
    "        )\n",
    "\n",
    "        supervised_loss = outputs.loss.mean()\n",
    "        \n",
    "        # ======== RL Loss ========\n",
    "        if i % rl_every_n_batches == 0:\n",
    "            \n",
    "            # Compute embeddings for the questions\n",
    "            output = model.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = output[0]  # Since it's a tuple with only one element\n",
    "            question_hidden_states = last_hidden_state.detach().cpu().numpy()  # ✅ Works for 2D tensors\n",
    "\n",
    "            n_docs = 5  # Number of documents to retrieve\n",
    "            \n",
    "            # Retrieve top 5 docs using embeddings\n",
    "            _, _, doc_dicts = retriever.retrieve(question_hidden_states, n_docs=n_docs)\n",
    "\n",
    "            query_terms = [rag_tokenizer.decode(ids, skip_special_tokens=True).split() for ids in input_ids]\n",
    "            sparse_doc_dicts = []\n",
    "            for query in query_terms:\n",
    "                scores = bm25.get_scores(query)\n",
    "                top_doc_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:n_docs]\n",
    "                sparse_doc_dicts.extend([sparse_corpus[i] for i in top_doc_indices])\n",
    "\n",
    "            # Combine dense and sparse retrieved documents\n",
    "            combined_contexts = [doc[\"text\"] for doc in doc_dicts] + sparse_doc_dicts\n",
    "            flat_combined_contexts = [item for sublist in combined_contexts for item in sublist]\n",
    "\n",
    "            context_encodings = generator_tokenizer.batch_encode_plus(\n",
    "                flat_combined_contexts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Reshape to [batch_size, n_docs, seq_len]\n",
    "            context_input_ids = context_encodings['input_ids'].view(BATCH_SIZE, n_docs, -1)\n",
    "            context_attention_mask = context_encodings['attention_mask'].view(input_ids.size(0), 5, -1).to(device)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_return_sequences=1,\n",
    "                num_beams=4,\n",
    "                n_docs=4,  # ← So that Batch Size is divisible by n_docs\n",
    "                max_length=16\n",
    "            )\n",
    "\n",
    "            generated_texts = generator_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            reference_texts = generator_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Prepare retrieved contexts\n",
    "            retrieved_contexts = []\n",
    "            for docs in doc_dicts:  # doc_dicts is batch of [n_docs retrieved per question]\n",
    "                contexts = [doc[\"text\"] for doc in doc_dicts]\n",
    "                flat_contexts = [item for sublist in contexts for item in sublist]\n",
    "                retrieved_contexts.append(flat_contexts)\n",
    "\n",
    "            # Compute reward\n",
    "            rewards = compute_reward(\n",
    "                generated_texts, \n",
    "                reference_texts, \n",
    "                retrieved_contexts=retrieved_contexts, \n",
    "                device=device)\n",
    "                        \n",
    "            # Compute log-probs manually\n",
    "            decoder_input_ids = generated_ids[:, :-1].contiguous()\n",
    "            labels_for_logprob = generated_ids[:, 1:].contiguous()\n",
    "\n",
    "            model_outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                context_input_ids=context_input_ids,\n",
    "                context_attention_mask=context_attention_mask,\n",
    "                use_cache=False\n",
    "            )\n",
    "\n",
    "            logits = model_outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_log_probs = torch.gather(log_probs, dim=2, index=labels_for_logprob.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "            # Mask padding\n",
    "            label_mask = (labels_for_logprob != model.config.pad_token_id).float()\n",
    "            sequence_log_probs = (token_log_probs * label_mask).sum(dim=1) / label_mask.sum(dim=1)\n",
    "\n",
    "            rl_loss = (-rewards * sequence_log_probs).mean()\n",
    "\n",
    "        else:\n",
    "            rl_loss = 0.0\n",
    "            \n",
    "        # ======== Combined Loss ========\n",
    "        combined_loss = Lambda * supervised_loss + (1 - Lambda) * rl_loss\n",
    "        combined_loss = combined_loss / accumulation_steps\n",
    "        combined_loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += combined_loss.item()\n",
    "   \n",
    "    print(f\"Epoch {epoch+1} - Combined Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"rag_model_rl_hr_finetuned\")        \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
